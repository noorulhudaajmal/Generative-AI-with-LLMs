{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c4f870-a593-400b-930c-5bd4f925c167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "pathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\n",
      "pathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.15.0 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.0a7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07036c35-4051-4d9c-beb9-6b0acc1a563c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde05496-5eab-4676-a3ed-eb5ce7a32575",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d94eab331441959319d622cc637d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141c80d203d740e5b32ce1094b30c5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526a067a99e74aa28177cbb85dc5f5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c6f9ffc1aa4b7894a61222334f2108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1f2634dfeb4762bd60e6064b44d8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f403c58cf24ed29bd3eeccade94ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4512ae2f9a64617b76c870e51a58a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "044903a0-c316-4e15-9cd1-6072e4a01f60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'dialogue', 'summary', 'topic'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a049946-d6a5-459d-ac33-7638787aaeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example# 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input Dialogue:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example# 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input Dialogue:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    print(f\"Example# {i}\\n{'-'*100}\")\n",
    "    print(f\"Input Dialogue:\\n{dataset['test'][i]['dialogue']}\\n\")\n",
    "    print(f\"Summary:\\n{dataset['test'][i]['summary']}\")\n",
    "    print(f\"{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4d3795-ef04-47e1-952b-f7b6644c1ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd53ff613d843c38309d197c9674570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8775c4b5ae8d4cf2b472d7edf1b1b28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e769335e1a44d9a725fb6c2783defa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee3024e-ba45-44f0-8ccb-1045e6809202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e384d2e7c20d4d5bbdeb40343b711679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8af2826f4b4429898d20276f426c147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed601d238944fccbaadc3f0db3a2188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4db8ecd841c467d82b42fea9aa01067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dac63744-df84-4f1e-803f-1d0e15f0c449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence: tensor([ 466, 2993,  248,    5, 1333,    5,    1])\n",
      "\n",
      "Decoded Sentence: That sounds great. Thanks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(text = \"That sounds great. Thanks.\", return_tensors='pt')\n",
    "\n",
    "decoded = tokenizer.decode(token_ids= encoded['input_ids'][0],\n",
    "                           skip_special_tokens=True)\n",
    "\n",
    "print(f'Encoded Sentence: {encoded[\"input_ids\"][0]}\\n')\n",
    "print(f'Decoded Sentence: {decoded}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845b8a4-3002-42be-b741-6f398d44b809",
   "metadata": {},
   "source": [
    "# INITIAL MODEL EVALUATION - NO PROMPT ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03ea48ba-6607-498c-ab02-5e8e28b607af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example# 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input Dialogue:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "\n",
      "Summary:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "Model Generated Summary:\n",
      "Brian, thank you for coming to our party.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example# 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input Dialogue:\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "\n",
      "Summary:\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "\n",
      "Model Generated Summary:\n",
      "#Person1#: Yeah.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, i in enumerate([10,50]):\n",
    "    input_dialogue = dataset['test'][i]['dialogue']\n",
    "    summary = dataset['test'][i]['summary']\n",
    "    \n",
    "    input_tokens = tokenizer(text=input_dialogue,\n",
    "                             return_tensors='pt')\n",
    "    output_tokens = model.generate(inputs=input_tokens['input_ids'],\n",
    "                                   max_new_tokens=50)\n",
    "    \n",
    "    output_summary = tokenizer.decode(token_ids=output_tokens[0],\n",
    "                                      skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Example# {num+1}\\n{'-'*100}\")\n",
    "    \n",
    "    print(f\"Input Dialogue:\\n{dataset['test'][i]['dialogue']}\\n\")\n",
    "    print(f\"Summary:\\n{dataset['test'][i]['summary']}\\n\")\n",
    "    print(f\"Model Generated Summary:\\n{output_summary}\")\n",
    "    \n",
    "    print(f\"{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de2d7c1-2a8e-4a83-90c3-b40a8dd902eb",
   "metadata": {},
   "source": [
    "Responses are not that good.\n",
    "- In the first example it seems like model is generating the next sentence for the dialogue.\n",
    "- Resposne in the second example does not fit to our need at all.\n",
    "- We can consider prompt engineering for summary generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd63773-a370-4758-8297-45c2a5b543d4",
   "metadata": {},
   "source": [
    "# PROMPT ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cbba7-1063-4518-876d-c06f02a02dcb",
   "metadata": {},
   "source": [
    "#### ZERO-SHOT INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67877e-bcda-4ec4-b9a1-1e54f9e4f02f",
   "metadata": {},
   "source": [
    "In this type of in-context learning, we can consider adding an instruction in the prompt for the task to be accomplished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "095a0d0f-2dff-4a78-9c53-c6032c98495d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example# 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prompt:\n",
      "Give a summary of the following dialogue:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "Summary:\n",
      "\n",
      "Summary:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "Model Generated Summary:\n",
      "Brian's birthday is coming up.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example# 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prompt:\n",
      "Give a summary of the following dialogue:\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "Summary:\n",
      "\n",
      "Summary:\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "\n",
      "Model Generated Summary:\n",
      "Person1 is going to make a few enemies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, i in enumerate([10,50]):\n",
    "    input_dialogue = dataset['test'][i]['dialogue']\n",
    "    summary = dataset['test'][i]['summary']\n",
    "    \n",
    "    prompt = f\"Give a summary of the following dialogue:\\n{input_dialogue}\\nSummary:\"\n",
    "    input_tokens = tokenizer(text=prompt,\n",
    "                             return_tensors='pt')\n",
    "    output_tokens = model.generate(inputs=input_tokens['input_ids'],\n",
    "                                   max_new_tokens=50)\n",
    "    \n",
    "    output_summary = tokenizer.decode(token_ids=output_tokens[0],\n",
    "                                      skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Example# {num+1}\\n{'-'*100}\")\n",
    "    \n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "    print(f\"Summary:\\n{dataset['test'][i]['summary']}\\n\")\n",
    "    print(f\"Model Generated Summary:\\n{output_summary}\")\n",
    "    \n",
    "    print(f\"{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47697ba-273d-46f5-bde7-5396bac4cf05",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model is understanding the context well but unable to precise conslusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390cf67b-3e7e-44b0-bad9-fe846f75e4e5",
   "metadata": {},
   "source": [
    "## ZERO SHOT INFERENCE WITH PRE-BUILT FLAN-T5 PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b009be4-e600-478f-8b76-b2af888f576f",
   "metadata": {},
   "source": [
    "Let's try one of the prompt from the FLAN-T5 prompt templates and see if model could pick up on the nuance of the dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bef3919c-d50d-40ad-a65c-58dba5bfe6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example# 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prompt:\n",
      "Dialogue:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "What was going on?\n",
      "\n",
      "Summary:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "Model Generated Summary:\n",
      "Brian's birthday is coming up.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example# 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prompt:\n",
      "Dialogue:\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "What was going on?\n",
      "\n",
      "Summary:\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "\n",
      "Model Generated Summary:\n",
      "Person1 is going to make a few enemies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, i in enumerate([10,50]):\n",
    "    input_dialogue = dataset['test'][i]['dialogue']\n",
    "    summary = dataset['test'][i]['summary']\n",
    "    \n",
    "    prompt = f\"Dialogue:\\n{input_dialogue}\\nWhat was going on?\"\n",
    "    input_tokens = tokenizer(text=prompt,\n",
    "                             return_tensors='pt')\n",
    "    output_tokens = model.generate(inputs=input_tokens['input_ids'],\n",
    "                                   max_new_tokens=50)\n",
    "    \n",
    "    output_summary = tokenizer.decode(token_ids=output_tokens[0],\n",
    "                                      skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Example# {num+1}\\n{'-'*100}\")\n",
    "    \n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "    print(f\"Summary:\\n{dataset['test'][i]['summary']}\\n\")\n",
    "    print(f\"Model Generated Summary:\\n{output_summary}\")\n",
    "    \n",
    "    print(f\"{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ac65fc-115c-4073-af54-e21ef2f5cdbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Unfortunetly, the model behaviour stays the same. We could consider improving it using the one-shot inference.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa596f9-3128-4e7a-890f-443943066447",
   "metadata": {},
   "source": [
    "## ONE SHOT INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17e98bee-ba2e-4600-a82a-f4350320c9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "What was going on?\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "Dialogue:\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "What was going on? \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Model Generated Summary:\n",
      "Person1 wants to make friends with two ugly old ladies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_example_index = 10\n",
    "test_evaluation_index = 50\n",
    "example_dialogue = dataset['test'][test_example_index]['dialogue']\n",
    "input_dialogue = dataset['test'][test_evaluation_index]['dialogue']\n",
    "example_summary = dataset['test'][test_example_index]['summary']\n",
    "summary = dataset['test'][test_evaluation_index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Dialogue:\\n{example_dialogue}\n",
    "What was going on?\\n{example_summary}\n",
    "Dialogue:\\n{input_dialogue}\n",
    "What was going on? \"\"\"\n",
    "\n",
    "input_tokens = tokenizer(text=prompt,\n",
    "                         return_tensors='pt')\n",
    "output_tokens = model.generate(inputs=input_tokens['input_ids'],\n",
    "                               max_new_tokens=50)\n",
    "\n",
    "output_summary = tokenizer.decode(token_ids=output_tokens[0],\n",
    "                                  skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"{'-'*100}\\n\")\n",
    "\n",
    "print(f\"Summary:\\n{dataset['test'][test_evaluation_index]['summary']}\\n\")\n",
    "print(f\"{'-'*100}\\n\")\n",
    "\n",
    "print(f\"Model Generated Summary:\\n{output_summary}\")\n",
    "\n",
    "print(f\"{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b8eec-883c-455e-8f09-e2f11d87fd2a",
   "metadata": {},
   "source": [
    "We can't see much better performance here as well. Let's change the prompt a bit and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "118c0493-a3bd-42ae-bb08-23fbbb263faf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "Give a summary of the following dialogue:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "Summary:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "Give a summary of the following dialogue:\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "Summary:\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Model Generated Summary:\n",
      "#Person1: I'm going to get a strip for my friend.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_example_index = 10\n",
    "test_evaluation_index = 50\n",
    "example_dialogue = dataset['test'][test_example_index]['dialogue']\n",
    "input_dialogue = dataset['test'][test_evaluation_index]['dialogue']\n",
    "example_summary = dataset['test'][test_example_index]['summary']\n",
    "summary = dataset['test'][test_evaluation_index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Give a summary of the following dialogue:\\n{example_dialogue}\n",
    "Summary:\\n{example_summary}\n",
    "Give a summary of the following dialogue:\\n{input_dialogue}\n",
    "Summary:\"\"\"\n",
    "\n",
    "input_tokens = tokenizer(text=prompt,\n",
    "                         return_tensors='pt')\n",
    "output_tokens = model.generate(inputs=input_tokens['input_ids'],\n",
    "                               max_new_tokens=50)\n",
    "\n",
    "output_summary = tokenizer.decode(token_ids=output_tokens[0],\n",
    "                                  skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"{'-'*100}\\n\")\n",
    "\n",
    "print(f\"Summary:\\n{dataset['test'][test_evaluation_index]['summary']}\\n\")\n",
    "print(f\"{'-'*100}\\n\")\n",
    "\n",
    "print(f\"Model Generated Summary:\\n{output_summary}\")\n",
    "\n",
    "print(f\"{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d20934-72ee-428e-92cb-fab305388f6d",
   "metadata": {},
   "source": [
    "The model still not generate summary upto the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2dcba1-ab2e-4d11-b1c4-5af51cba931a",
   "metadata": {},
   "source": [
    "## FEW SHOT INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6a2cd5d5-9274-4ca6-82f9-cfa944ec20cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "    Dialogue:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "    What was going on?\n",
      "Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.\n",
      "    \n",
      "    Dialogue:\n",
      "#Person1#: What's wrong with you? Why are you scratching so much?\n",
      "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
      "#Person1#: Let me have a look. Whoa! Get away from me!\n",
      "#Person2#: What's wrong?\n",
      "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
      "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
      "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
      "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
      "    What was going on?\n",
      "#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n",
      "    Dialogue:\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "What was going on?\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Model Generated Summary:\n",
      "#Person1 wants to make a few enemies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_example_indices = [2, 20]\n",
    "test_evaluation_index = 50\n",
    "input_dialogue = dataset['test'][test_evaluation_index]['dialogue']\n",
    "summary = dataset['test'][test_evaluation_index]['summary']\n",
    "\n",
    "prompt = \"\"\n",
    "for i in test_example_indices:\n",
    "    prompt += f\"\"\"\n",
    "    Dialogue:\\n{dataset['test'][i]['dialogue']}\n",
    "    What was going on?\\n{dataset['test'][i]['summary']}\n",
    "    \"\"\"\n",
    "prompt += f\"\"\"Dialogue:\\n{input_dialogue}\n",
    "What was going on?\"\"\"\n",
    "\n",
    "input_tokens = tokenizer(text=prompt,\n",
    "                         return_tensors='pt')\n",
    "output_tokens = model.generate(inputs=input_tokens['input_ids'],\n",
    "                               max_new_tokens=50)\n",
    "\n",
    "output_summary = tokenizer.decode(token_ids=output_tokens[0],\n",
    "                                  skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"{'-'*100}\\n\")\n",
    "\n",
    "print(f\"Summary:\\n{dataset['test'][test_evaluation_index]['summary']}\\n\")\n",
    "print(f\"{'-'*100}\\n\")\n",
    "\n",
    "print(f\"Model Generated Summary:\\n{output_summary}\")\n",
    "\n",
    "print(f\"{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0bbbac7-d7aa-4771-816b-0caba7535ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "848"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe2faf-45b2-4fdb-af48-f9c151207036",
   "metadata": {},
   "source": [
    "In few shot inference, the model do not generate any better. We can conclude one-shot learnign to be more effective than any othere here.\n",
    "\n",
    "Another thing we can do is to change the configuration parameters of the `model.generate()` fucntion to see variability in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82660ddb-d55c-405e-b100-baa9e61eae37",
   "metadata": {},
   "source": [
    "# Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "da9387ce-f6f5-4598-8438-a822f6284caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3088caff-b787-4a6f-8467-5de52f992853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['In-Context Learning Type',\n",
    "                                  'prompt_template',\n",
    "                                  'max_new_tokens', 'do_sample', \n",
    "                                  'temperature','Human Generated Summary',\n",
    "                                  'Model Generate Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0dd857c6-b457-4b20-8d14-6b2b4e22d071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic = {\n",
    "  'In-Context Learning Type':1,\n",
    "  'prompt_template':2,\n",
    "  'max_new_tokens':3, 'do_sample':3, \n",
    "  'temperature':3,'Human Generated Summary':2,\n",
    "  'Model Generate Summary':2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "f0017440-9d68-4d3d-a8dc-44deb3f9c7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt(inference_type:str,\n",
    "                  prompt_template=None,\n",
    "                  testing_indices=[10],\n",
    "                  evaluation_index=100):\n",
    "    \n",
    "    input_dialogue = dataset['test'][evaluation_index]['dialogue']\n",
    "    \n",
    "    if inference_type == 'zero-shot':\n",
    "        return input_dialogue\n",
    "    elif inference_type == 'one-shot':\n",
    "        testing_indices = [testing_indices[0]]\n",
    "    \n",
    "    prompt_start = prompt_template.split(\"{\")[0]\n",
    "    prompt_end = prompt_template.split(\"}\")[1]\n",
    "    prompt = \"\"\n",
    "    for i in testing_indices:\n",
    "        prompt += f\"{prompt_start}\\n{dataset['test'][i]['dialogue']}\\n{prompt_end}\\n{dataset['test'][i]['summary']}\\n\"\n",
    "        prompt +=\"\\n\"\n",
    "    \n",
    "    prompt += f\"{prompt_start}\\n{input_dialogue}\\n{prompt_end}\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "be15918c-d426-416d-a66d-fe2a8164a140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_1 = \"Dialogue:{dialogue}\\nWhats going on?\"\n",
    "template_2 = \"Summarize the following conversation:{dialogue}\\nSummary:\"\n",
    "template_3 = \"Provide summary:{dialogue}\\nSummary:\"\n",
    "\n",
    "templates = [template_1, template_2, template_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f71c3a98-59b4-4a96-8e2b-95ea4c80183d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_new_tokens':[10,50],\n",
    "    'do_sample':[True],\n",
    "    'temperature':[None, 0.1, 0.5, 1.0]\n",
    "}\n",
    "param_combinations = list(itertools.product(*params.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d34f7-520d-4f0b-a194-016df9b65092",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Let's first re-generate the zero-shot examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "27461d54-7f58-45f5-b217-eba8fb22d827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/1772620600.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'zero-shot',\n",
      "/tmp/ipykernel_21/1772620600.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'zero-shot',\n",
      "/tmp/ipykernel_21/1772620600.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'zero-shot',\n",
      "/tmp/ipykernel_21/1772620600.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'zero-shot',\n",
      "/tmp/ipykernel_21/1772620600.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'zero-shot',\n",
      "/tmp/ipykernel_21/1772620600.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'zero-shot',\n",
      "/tmp/ipykernel_21/1772620600.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'zero-shot',\n",
      "/tmp/ipykernel_21/1772620600.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'zero-shot',\n"
     ]
    }
   ],
   "source": [
    "evaluation_index = 100\n",
    "prompt = create_prompt(inference_type=\"zero-shot\",\n",
    "                       evaluation_index=evaluation_index)\n",
    "\n",
    "input_tokens = tokenizer(text=prompt,\n",
    "                         return_tensors='pt')\n",
    "actual_summary = dataset['test'][evaluation_index]['summary']\n",
    "\n",
    "for combination in param_combinations:\n",
    "    param_dict = dict(zip(params.keys(), combination))\n",
    "    output_tokens = model.generate(inputs=input_tokens['input_ids'],\n",
    "                                   generation_config=GenerationConfig(\n",
    "                                       max_new_tokens=param_dict['max_new_tokens'],\n",
    "                                       do_sample=param_dict['do_sample'],\n",
    "                                       temperature=param_dict['temperature']\n",
    "                                   ))\n",
    "    output_summary = tokenizer.decode(token_ids=output_tokens[0],\n",
    "                                      skip_special_tokens=True)\n",
    "\n",
    "    results = results.append({'In-Context Learning Type':'zero-shot',\n",
    "                    'prompt_template':\"-\",\n",
    "                    'max_new_tokens':param_dict['max_new_tokens'], \n",
    "                    'do_sample':param_dict['do_sample'],\n",
    "                    'temperature':param_dict['temperature'],\n",
    "                    'Human Generated Summary':actual_summary,\n",
    "                    'Model Generate Summary':output_summary\n",
    "                   }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce1316-a8eb-43db-8be9-dbdd184aa91b",
   "metadata": {},
   "source": [
    "**Now we will generate one shot inferences and then move on to few-shot inferences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c9b5f1f2-c994-4e3a-b154-05dbad6ef620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n",
      "/tmp/ipykernel_21/720667955.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'one-shot',\n"
     ]
    }
   ],
   "source": [
    "evaluation_index = 100\n",
    "for template in templates:\n",
    "    prompt = create_prompt(inference_type=\"one-shot\",\n",
    "                           prompt_template=template,\n",
    "                           testing_indices=[10],\n",
    "                           evaluation_index=evaluation_index)\n",
    "\n",
    "    input_tokens = tokenizer(text=prompt,\n",
    "                             return_tensors='pt')\n",
    "    actual_summary = dataset['test'][evaluation_index]['summary']\n",
    "    \n",
    "    for combination in param_combinations:\n",
    "        param_dict = dict(zip(params.keys(), combination))\n",
    "        output_tokens = model.generate(inputs=input_tokens['input_ids'],\n",
    "                                       generation_config=GenerationConfig(\n",
    "                                           max_new_tokens=param_dict['max_new_tokens'],\n",
    "                                           do_sample=param_dict['do_sample'],\n",
    "                                           temperature=param_dict['temperature']\n",
    "                                       ))\n",
    "        output_summary = tokenizer.decode(token_ids=output_tokens[0],\n",
    "                                          skip_special_tokens=True)\n",
    "        \n",
    "        results = results.append({'In-Context Learning Type':'one-shot',\n",
    "                        'prompt_template':template,\n",
    "                        'max_new_tokens':param_dict['max_new_tokens'], \n",
    "                        'do_sample':param_dict['do_sample'],\n",
    "                        'temperature':param_dict['temperature'],\n",
    "                        'Human Generated Summary':actual_summary,\n",
    "                        'Model Generate Summary':output_summary\n",
    "                       }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e8d5ba57-85d5-475e-8912-8af8d3fe317f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n",
      "/tmp/ipykernel_21/3121368368.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({'In-Context Learning Type':'few-shot',\n"
     ]
    }
   ],
   "source": [
    "evaluation_index = 100\n",
    "for template in templates:\n",
    "    prompt = create_prompt(inference_type=\"few-shot\",\n",
    "                           prompt_template=template,\n",
    "                           testing_indices=[10,60],\n",
    "                           evaluation_index=evaluation_index)\n",
    "\n",
    "    input_tokens = tokenizer(text=prompt,\n",
    "                             return_tensors='pt')\n",
    "    actual_summary = dataset['test'][evaluation_index]['summary']\n",
    "    \n",
    "    for combination in param_combinations:\n",
    "        param_dict = dict(zip(params.keys(), combination))\n",
    "        output_tokens = model.generate(inputs=input_tokens['input_ids'],\n",
    "                                       generation_config=GenerationConfig(\n",
    "                                           max_new_tokens=param_dict['max_new_tokens'],\n",
    "                                           do_sample=param_dict['do_sample'],\n",
    "                                           temperature=param_dict['temperature']\n",
    "                                       ))\n",
    "        output_summary = tokenizer.decode(token_ids=output_tokens[0],\n",
    "                                          skip_special_tokens=True)\n",
    "        \n",
    "        results = results.append({'In-Context Learning Type':'few-shot',\n",
    "                        'prompt_template':template,\n",
    "                        'max_new_tokens':param_dict['max_new_tokens'], \n",
    "                        'do_sample':param_dict['do_sample'],\n",
    "                        'temperature':param_dict['temperature'],\n",
    "                        'Human Generated Summary':actual_summary,\n",
    "                        'Model Generate Summary':output_summary\n",
    "                       }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "bacf4f1d-b08c-410e-a3ac-9cb097eab50f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>In-Context Learning Type</th>\n",
       "      <th>prompt_template</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>Human Generated Summary</th>\n",
       "      <th>Model Generate Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zero-shot</td>\n",
       "      <td>Dialogue:{dialogue}\\nWhats going on?</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>#Person1# and Mike have a disagreement on how ...</td>\n",
       "      <td>#Person1#: This is a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  In-Context Learning Type                       prompt_template  \\\n",
       "0                        1                                     2   \n",
       "1                zero-shot  Dialogue:{dialogue}\\nWhats going on?   \n",
       "\n",
       "  max_new_tokens do_sample temperature  \\\n",
       "0              3         3           3   \n",
       "1             10      True        None   \n",
       "\n",
       "                             Human Generated Summary Model Generate Summary  \n",
       "0                                                  2                      2  \n",
       "1  #Person1# and Mike have a disagreement on how ...   #Person1#: This is a  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e3b722cc-2efc-4909-9700-b362231e4eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person2#: Mike, how the others react, and try to understand them.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: I'm sorry to hear that. I'm sorry to hear that. I'm sorry to hear that. I'm sorry to hear that. I'm sorry to hear that. I'm sorry to\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: So, what was the problem that time? #Person2#: She's telling me that she doesn't want to see me anymore. #Person1#: I'm acting hurt and sad\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Let's try this question from Person1: Which situation did you make wrong with Jason's reaction?\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Why did Jason and Laura have such a bad\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.1\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: I'm sorry to\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.5\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: I think Jason's\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t1.0\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "First, make the lines for both parties. \n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Person1: OK, everyone.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: I'm sorry to hear that. I'm sorry to hear that. I'm sorry to hear that. I'm sorry to hear that. I'm sorry to hear that. I'm sorry\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Person1: I'm sorry to hear that. I'm sure it was a mistake. If it wasn't, it wasn't going to be a good one. I'm not sure about that. #Person\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two men have a cut on their butts.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "I have to be very angry because you're\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.1\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: Mike, I'm\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.5\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: I'm not\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t1.0\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "All right, so we will look at this.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: Mike has an angry face and Laura's not. #Person2: Jason has an angry face and Laura's not. #Person1: Okay, that's how it goes.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: I'm sorry, but I'm not sure what Jason and Laura are doing.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: Mike, I'm angry that you are acting so wrong. I want to see what Jason and Laura could do to make Jason feel better. #Person2#: I don't think Jason would react the\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "We don't want to mess around right now, especially when you're struggling. We want help if you want, but there are situations in which this problem has been difficult.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Parent1: OK, I agree with\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Answer: At this point Mike and Laura are acting\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.1\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: I'm sorry,\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.5\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: I'm \n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t1.0\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: We're trying\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: Okay, you're a cut! #Person2: OK. I told you this. #Person1: OK. I'm not sure. #Person2: OK. I'm just\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: I'm sorry to hear that. I'm sorry to hear that. I'm sorry to hear that. I'm sorry to hear that. I'm sorry to hear that. I'm sorry to\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two talk about how they feel when they say their lines.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "If your reaction isn't right, it's not even bad.\n",
      "****************************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_shot_inf = results[results['In-Context Learning Type']=='zero-shot']\n",
    "for i in zero_shot_inf[20:].iterrows():\n",
    "    print(f\"max_new_tokens:\\t{i[1]['max_new_tokens']}\")\n",
    "    print(f\"temperature:\\t{i[1]['temperature']}\")\n",
    "    print(f\"Human Generated Summary:\\n{i[1]['Human Generated Summary']}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Model Generate Summary:\\n{i[1]['Model Generate Summary']}\")\n",
    "    print(f\"{'*'*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d02b4-67e0-4c2a-a74e-524c51ade1f3",
   "metadata": {},
   "source": [
    "## ONE SHOT INFERENCE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "473c3ea2-0923-4371-b66d-b2e3457e3793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "People discuss how Mike responded to a joke and\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.1\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two people are trying to figure out how to\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.5\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The problem Mike is having with Laura is that she\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t1.0\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person2 tells her to keep trying\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "It feels like a cut from a sarcasm to Mike.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two people are trying to figure out how to react to a cut.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Person1 is angry at Mike for letting her be angry and sad. She doesn't want to see Mike anymore. Jason and Laura have been together for three years.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "People think Jason and Laura's reaction at first, and they're not convinced about it later.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Jason with Laura is putting in a long\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.1\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two of them are trying to figure out how\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.5\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: Mike is angry with\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t1.0\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The cut that's been made comes out of\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The cut can't be exactly as it should be. There are ways of working it out.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two of them are going to try to figure out how to react to Jason's reaction.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Talk to Mike about what was wrong with Jason.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "When Jason and Laura were on the set of an episode of \"Glamour\", they were excited about being together and taking good care of each other.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Everyone shares their feelings on getting the cut.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.1\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: Mike is angry at\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.5\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Mike wants to get more anger from his girlfriend.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t1.0\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person2#: Jason, you'\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "When Mike utters his lines in one form or another, the person he's talking to finds the first one most accurate.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: Mike is angry with Laura. #Person2: Jason is angry with Laura. #Person3: Jason is angry with Laura. #Person4: Jason is angry with Laura. #Person5: Jason\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: I'm trying to talk to Mike.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Person1's addressing the problem at the DWTS in the workplace.\n",
      "****************************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_shot_inf = results[results['In-Context Learning Type']=='one-shot']\n",
    "for i in one_shot_inf.iterrows():\n",
    "    print(f\"max_new_tokens:\\t{i[1]['max_new_tokens']}\")\n",
    "    print(f\"temperature:\\t{i[1]['temperature']}\")\n",
    "    print(f\"prompt_template:\\t{i[1]['prompt_template']}\")\n",
    "    print(f\"Human Generated Summary:\\n{i[1]['Human Generated Summary']}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Model Generate Summary:\\n{i[1]['Model Generate Summary']}\")\n",
    "    print(f\"{'*'*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f2963-01d2-451d-be48-bafb86d605cb",
   "metadata": {},
   "source": [
    "## FEW SHOT INFERENCE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "59324f9f-7e38-478f-aef4-afca4c433e77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "People discuss how Mike responded to a joke and\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.1\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two people are trying to figure out how to\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.5\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The problem Mike is having with Laura is that she\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t1.0\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person2 tells her to keep trying\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "It feels like a cut from a sarcasm to Mike.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two people are trying to figure out how to react to a cut.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Person1 is angry at Mike for letting her be angry and sad. She doesn't want to see Mike anymore. Jason and Laura have been together for three years.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "prompt_template:\tDialogue:{dialogue}\n",
      "Whats going on?\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "People think Jason and Laura's reaction at first, and they're not convinced about it later.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Jason with Laura is putting in a long\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.1\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two of them are trying to figure out how\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.5\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: Mike is angry with\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t1.0\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The cut that's been made comes out of\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The cut can't be exactly as it should be. There are ways of working it out.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "The two of them are going to try to figure out how to react to Jason's reaction.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Talk to Mike about what was wrong with Jason.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "prompt_template:\tSummarize the following conversation:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "When Jason and Laura were on the set of an episode of \"Glamour\", they were excited about being together and taking good care of each other.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\tNone\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Everyone shares their feelings on getting the cut.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.1\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: Mike is angry at\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t0.5\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Mike wants to get more anger from his girlfriend.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t10\n",
      "temperature:\t1.0\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person2#: Jason, you'\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\tNone\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "When Mike utters his lines in one form or another, the person he's talking to finds the first one most accurate.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.1\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1: Mike is angry with Laura. #Person2: Jason is angry with Laura. #Person3: Jason is angry with Laura. #Person4: Jason is angry with Laura. #Person5: Jason\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t0.5\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "#Person1#: I'm trying to talk to Mike.\n",
      "****************************************************************************************************\n",
      "\n",
      "max_new_tokens:\t50\n",
      "temperature:\t1.0\n",
      "prompt_template:\tProvide summary:{dialogue}\n",
      "Summary:\n",
      "Human Generated Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "\n",
      "Model Generate Summary:\n",
      "Person1's addressing the problem at the DWTS in the workplace.\n",
      "****************************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_inf = results[results['In-Context Learning Type']=='few-shot']\n",
    "for i in one_shot_inf.iterrows():\n",
    "    print(f\"max_new_tokens:\\t{i[1]['max_new_tokens']}\")\n",
    "    print(f\"temperature:\\t{i[1]['temperature']}\")\n",
    "    print(f\"prompt_template:\\t{i[1]['prompt_template']}\")\n",
    "    print(f\"Human Generated Summary:\\n{i[1]['Human Generated Summary']}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Model Generate Summary:\\n{i[1]['Model Generate Summary']}\")\n",
    "    print(f\"{'*'*100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05160288-1d1f-4d06-a5b8-7e1e34822930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
